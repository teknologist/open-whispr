import { useState, useEffect, useRef } from "react";
import AudioManager from "../helpers/audioManager";

// Delay before hiding window after transcription completes (ms)
// This provides visual feedback that transcription was successful
const HIDE_WINDOW_DELAY_MS = 300;

export const useAudioRecording = (toast, options = {}) => {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcript, setTranscript] = useState("");
  const audioManagerRef = useRef(null);
  const { onToggle } = options;

  useEffect(() => {
    // Initialize AudioManager
    audioManagerRef.current = new AudioManager();

    // Set up callbacks
    audioManagerRef.current.setCallbacks({
      onStateChange: ({ isRecording, isProcessing }) => {
        setIsRecording(isRecording);
        setIsProcessing(isProcessing);

        // Show overlay when recording starts, hide when done
        if (isRecording || isProcessing) {
          window.electronAPI?.showDictationPanel?.();
        }
      },
      onError: (error) => {
        toast({
          title: error.title,
          description: error.description,
          variant: "destructive",
        });
      },
      onTranscriptionComplete: async (result) => {
        if (result.success) {
          setTranscript(result.text);

          // Paste immediately
          await audioManagerRef.current.safePaste(result.text);

          // Save to database in parallel
          audioManagerRef.current.saveTranscription(result.text);

          // Show success notification if local fallback was used
          if (
            result.source === "openai" &&
            localStorage.getItem("useLocalWhisper") === "true"
          ) {
            toast({
              title: "Fallback Mode",
              description: "Local Whisper failed. Used OpenAI API instead.",
              variant: "default",
            });
          }
        }

        // Hide overlay after transcription (with brief delay for visual feedback)
        setTimeout(() => {
          window.electronAPI?.hideWindow?.();
        }, HIDE_WINDOW_DELAY_MS);
      },
    });

    // Set up hotkey listener
    let recording = false;
    const handleToggle = () => {
      const currentState = audioManagerRef.current.getState();

      if (
        !recording &&
        !currentState.isRecording &&
        !currentState.isProcessing
      ) {
        audioManagerRef.current.startRecording();
        recording = true;
      } else if (currentState.isRecording) {
        audioManagerRef.current.stopRecording();
        recording = false;
      }
    };

    const disposeToggle = window.electronAPI.onToggleDictation(() => {
      handleToggle();
      onToggle?.();
    });

    // Set up no-audio-detected listener
    const handleNoAudioDetected = () => {
      toast({
        title: "No Audio Detected",
        description:
          "The recording contained no detectable audio. Please try again.",
        variant: "default",
      });
    };

    const disposeNoAudio = window.electronAPI.onNoAudioDetected?.(
      handleNoAudioDetected,
    );

    // Cleanup
    return () => {
      disposeToggle?.();
      disposeNoAudio?.();
      if (audioManagerRef.current) {
        audioManagerRef.current.cleanup();
      }
    };
  }, [toast, onToggle]);

  const startRecording = async () => {
    if (audioManagerRef.current) {
      return await audioManagerRef.current.startRecording();
    }
    return false;
  };

  const stopRecording = () => {
    if (audioManagerRef.current) {
      return audioManagerRef.current.stopRecording();
    }
    return false;
  };

  const toggleListening = () => {
    if (!isRecording && !isProcessing) {
      startRecording();
    } else if (isRecording) {
      stopRecording();
    }
  };

  return {
    isRecording,
    isProcessing,
    transcript,
    startRecording,
    stopRecording,
    toggleListening,
  };
};
