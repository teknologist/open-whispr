# OpenWhispr

An open source desktop dictation application that converts speech to text using OpenAI Whisper. Features both local and cloud processing options for maximum flexibility and privacy.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. This means you can freely use, modify, and distribute this software for personal or commercial purposes.

## Features

- ðŸŽ¤ **Global Hotkey**: Customizable hotkey to start/stop dictation from anywhere (default: backtick `)
- ðŸ¤– **Multi-Provider AI Processing**: Choose between OpenAI, Anthropic Claude, Google Gemini, or local models
- ðŸŽ¯ **Agent Naming**: Personalize your AI assistant with a custom name for natural interactions
- ðŸ§  **Latest AI Models** (September 2025):
  - **OpenAI**: GPT-5 Series, GPT-4.1 Series, o-series reasoning models (o3/o4-mini)
  - **Anthropic**: Claude Opus 4.1, Claude Sonnet 4, Claude 3.5 Sonnet/Haiku
  - **Google**: Gemini 2.5 Pro/Flash/Flash-Lite with thinking capability, Gemini 2.0 Flash
  - **Local**: Qwen, LLaMA, Mistral models via llama.cpp
- ðŸ”’ **Privacy-First**: Local processing keeps your voice data completely private
- ðŸŽ¨ **Modern UI**: Built with React 19, TypeScript, and Tailwind CSS v4
- ðŸš€ **Fast**: Optimized with Vite and modern tooling
- ðŸ“± **Control Panel**: Manage settings, view history, and configure API keys
- ðŸ—„ï¸ **Transcription History**: SQLite database stores all your transcriptions locally
- ðŸ”§ **Model Management**: Download and manage local Whisper models (tiny, base, small, medium, large, turbo)
- ðŸ§¹ **Model Cleanup**: One-click removal of cached Whisper models with uninstall hooks to keep disks tidy
- ðŸŒ **Cross-Platform**: Works on macOS, Windows, and Linux
- âš¡ **Automatic Pasting**: Transcribed text automatically pastes at your cursor location
- ðŸ–±ï¸ **Draggable Interface**: Move the dictation panel anywhere on your screen
- ðŸ”„ **OpenAI Responses API**: Using the latest Responses API for improved performance
- ðŸŒ **Globe Key Toggle (macOS)**: Optional Fn/Globe key listener for a hardware-level dictation trigger

## Prerequisites

- **Node.js 18+** and npm (Download from [nodejs.org](https://nodejs.org/))
- **macOS 10.15+**, **Windows 10+**, or **Linux**
- On macOS, Globe key support requires the Xcode Command Line Tools (`xcode-select --install`) so the bundled Swift helper can run
- **Python 3.7+** (Optional - the app can install it automatically for local Whisper processing)

## Quick Start

### For Personal Use (Recommended)

1. **Clone the repository**:
   ```bash
   git clone https://github.com/HeroTools/open-whispr.git
   cd open-whispr
   ```

2. **Install dependencies**:
   ```bash
   npm install
   ```

3. **Optional: Set up API keys** (only needed for cloud processing):
   
   **Method A - Environment file**:
   ```bash
   cp env.example .env
   # Edit .env and add your API keys:
   # OPENAI_API_KEY=your_openai_key
   # ANTHROPIC_API_KEY=your_anthropic_key  
   # GEMINI_API_KEY=your_gemini_key
   ```
   
   **Method B - In-app configuration**:
   - Run the app and configure API keys through the Control Panel
   - Keys are automatically saved and persist across app restarts

4. **Run the application**:
   ```bash
   npm run dev  # Development mode with hot reload
   # OR
   npm start    # Production mode
   ```

### Building for Personal Use (Optional)

If you want to build a standalone app for personal use:

```bash
# Build without code signing (no certificates required)
npm run pack

# The unsigned app will be in: dist/mac-arm64/OpenWhispr.app (macOS)
# or dist/win-unpacked/OpenWhispr.exe (Windows)
# or dist/linux-unpacked/open-whispr (Linux)
```

**Note**: On macOS, you may see a security warning when first opening the unsigned app. Right-click and select "Open" to bypass this.

#### Linux (Multiple Package Formats)

OpenWhispr now supports multiple Linux package formats for maximum compatibility:

**Available Formats**:
- `.deb` - Debian, Ubuntu, Linux Mint, Pop!_OS
- `.rpm` - Fedora, Red Hat, CentOS, openSUSE
- `.tar.gz` - Universal archive (works on any distro)
- `.flatpak` - Sandboxed cross-distro package
- `AppImage` - Portable single-file executable

**Building Linux Packages**:

```bash
# Build default Linux package formats (AppImage, deb, rpm, tar.gz)
npm run build:linux

# Find packages in dist/:
# - OpenWhispr-x.x.x-linux-x64.AppImage
# - OpenWhispr-x.x.x-linux-x64.deb
# - OpenWhispr-x.x.x-linux-x64.rpm
# - OpenWhispr-x.x.x-linux-x64.tar.gz
```

**Optional: Building Flatpak** (requires additional setup):

```bash
# Install Flatpak build tools
sudo apt install flatpak flatpak-builder  # Debian/Ubuntu
# OR
sudo dnf install flatpak flatpak-builder  # Fedora/RHEL

# Add Flathub repository and install runtime
flatpak remote-add --user --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo
flatpak install --user -y flathub org.freedesktop.Platform//24.08 org.freedesktop.Sdk//24.08

# Add "flatpak" to linux.target in electron-builder.json, then build
npm run build:linux
```

**Installation Examples**:

```bash
# Debian/Ubuntu
sudo apt install ./dist/OpenWhispr-*-linux-x64.deb

# Fedora/RHEL
sudo dnf install ./dist/OpenWhispr-*-linux-x64.rpm

# Universal tar.gz (no root required)
tar -xzf dist/OpenWhispr-*-linux-x64.tar.gz
cd OpenWhispr-*/
./openwhispr

# Flatpak
flatpak install --user ./dist/OpenWhispr-*-linux-x64.flatpak

# AppImage (existing method)
chmod +x dist/OpenWhispr-*.AppImage
./dist/OpenWhispr-*.AppImage
```

**Optional Dependencies for Automatic Paste**:

The clipboard paste feature requires platform-specific tools:

**X11 (Traditional Linux Desktop)**:
```bash
# Debian/Ubuntu
sudo apt install xdotool

# Fedora/RHEL
sudo dnf install xdotool

# Arch
sudo pacman -S xdotool
```

**Wayland (Modern Linux Desktop)**:
```bash
# Debian/Ubuntu
sudo apt install wtype

# Fedora/RHEL
sudo dnf install wtype

# Arch
sudo pacman -S wtype
```

> â„¹ï¸ **Note**: wtype is required on Wayland for automatic pasting and supports UTF-8 text (accents, emojis). On X11, xdotool is used. If no paste tool is installed, text will still be copied to the clipboard - you'll just need to paste manually.

> ðŸ”’ **Flatpak Security**: The Flatpak package includes sandboxing with explicit permissions for microphone, clipboard, and file access. See [electron-builder.json](electron-builder.json) for the complete permission list.

### Building for Distribution

For maintainers who need to distribute signed builds:

```bash
# Requires code signing certificates and notarization setup
npm run build:mac    # macOS (requires Apple Developer account)
npm run build:win    # Windows (requires code signing cert)
npm run build:linux  # Linux
```

### First Time Setup

1. **Choose Processing Method**:
   - **Local Processing**: Download Whisper models for completely private transcription
   - **Cloud Processing**: Use OpenAI's API for faster transcription (requires API key)

2. **Grant Permissions**:
   - **Microphone Access**: Required for voice recording
   - **Accessibility Permissions**: Required for automatic text pasting (macOS)

3. **Name Your Agent**: Give your AI assistant a personal name (e.g., "Assistant", "Jarvis", "Alex")
   - Makes interactions feel more natural and conversational
   - Helps distinguish between giving commands and regular dictation
   - Can be changed anytime in settings

4. **Configure Global Hotkey**: Default is backtick (`) but can be customized

## Usage

### Basic Dictation
1. **Start the app** - A small draggable panel appears on your screen
2. **Press your hotkey** (default: backtick `) - Start dictating (panel shows recording animation)
3. **Press your hotkey again** - Stop dictation and begin transcription (panel shows processing animation)
4. **Text appears** - Transcribed text is automatically pasted at your cursor location
5. **Drag the panel** - Click and drag to move the dictation panel anywhere on your screen

### Control Panel
- **Access**: Right-click the tray icon (macOS) or through the system menu
- **Configure**: Choose between local and cloud processing
- **History**: View, copy, and delete past transcriptions
- **Models**: Download and manage local Whisper models
- **Storage Cleanup**: Remove downloaded Whisper models from cache to reclaim space
- **Settings**: Configure API keys, customize hotkeys, and manage permissions

### Uninstall & Cache Cleanup
- **In-App**: Use *Settings â†’ Speech to Text Processing â†’ Local Model Storage â†’ Remove Downloaded Models* to clear `~/.cache/openwhispr/models` (or `%USERPROFILE%\.cache\openwhispr\models` on Windows).
- **Windows Uninstall**: The NSIS uninstaller automatically deletes the same cache directory.
- **Linux Packages**: `deb`/`rpm` post-uninstall scripts also remove cached models.
- **macOS**: If you uninstall manually, remove `~/Library/Caches` or `~/.cache/openwhispr/models` if desired.

### Agent Naming & AI Processing
Once you've named your agent during setup, you can interact with it using multiple AI providers:

**ðŸŽ¯ Agent Commands** (for AI assistance):
- "Hey [AgentName], make this more professional"
- "Hey [AgentName], format this as a list"
- "Hey [AgentName], write a thank you email"
- "Hey [AgentName], convert this to bullet points"

**ðŸ¤– AI Provider Options**:
- **OpenAI**: 
  - GPT-5 Series (Nano/Mini/Full) - Latest generation with deep reasoning
  - GPT-4.1 Series - Enhanced coding with 1M token context
  - o3/o4 Series - Advanced reasoning models with longer thinking
- **Anthropic**: Claude Opus 4.1, Sonnet 4 - Frontier intelligence models
- **Google**: Gemini 2.5 Pro/Flash - Advanced multi-modal capabilities
- **Local**: Community models for complete privacy

**ðŸ“ Regular Dictation** (for normal text):
- "This is just normal text I want transcribed"
- "Meeting notes: John mentioned the quarterly report"
- "Dear Sarah, thank you for your help"

The AI automatically detects when you're giving it commands versus dictating regular text, and removes agent name references from the final output.

### Processing Options
- **Local Processing**: 
  - Install Whisper automatically through the Control Panel
  - Download models: tiny (fastest), base (recommended), small, medium, large (best quality)
  - Complete privacy - audio never leaves your device
- **Cloud Processing**:
  - Requires OpenAI API key
  - Faster processing
  - Uses OpenAI's Whisper API

## Project Structure

```
open-whispr/
â”œâ”€â”€ main.js              # Electron main process & IPC handlers
â”œâ”€â”€ preload.js           # Electron preload script & API bridge
â”œâ”€â”€ whisper_bridge.py    # Python script for local Whisper processing
â”œâ”€â”€ setup.js             # First-time setup script
â”œâ”€â”€ package.json         # Dependencies and scripts
â”œâ”€â”€ env.example          # Environment variables template
â”œâ”€â”€ CHANGELOG.md         # Project changelog
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.jsx          # Main dictation interface
â”‚   â”œâ”€â”€ main.jsx         # React entry point
â”‚   â”œâ”€â”€ index.html       # Vite HTML template
â”‚   â”œâ”€â”€ index.css        # Tailwind CSS v4 configuration
â”‚   â”œâ”€â”€ vite.config.js   # Vite configuration
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ControlPanel.tsx     # Settings and history UI
â”‚   â”‚   â”œâ”€â”€ OnboardingFlow.tsx   # First-time setup wizard
â”‚   â”‚   â”œâ”€â”€ SettingsPage.tsx     # Settings interface
â”‚   â”‚   â”œâ”€â”€ ui/                  # shadcn/ui components
â”‚   â”‚   â”‚   â”œâ”€â”€ button.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ card.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ input.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ LoadingDots.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DotFlashing.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Toast.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ toggle.tsx
â”‚   â”‚   â”‚   â””â”€â”€ tooltip.tsx
â”‚   â”‚   â””â”€â”€ lib/
â”‚   â”‚       â””â”€â”€ utils.ts         # Utility functions
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ ReasoningService.ts  # Multi-provider AI processing (OpenAI/Anthropic/Gemini)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ agentName.ts         # Agent name management utility
â”‚   â””â”€â”€ components.json          # shadcn/ui configuration
â””â”€â”€ assets/                      # App icons and resources
```

## Technology Stack

- **Frontend**: React 19, TypeScript, Tailwind CSS v4
- **Build Tool**: Vite with optimized Tailwind plugin
- **Desktop**: Electron 36 with context isolation
- **UI Components**: shadcn/ui with Radix primitives
- **Database**: better-sqlite3 for local transcription storage
- **Speech-to-Text**: OpenAI Whisper (local models + API)
- **Local Processing**: Python with OpenAI Whisper package
- **Icons**: Lucide React for consistent iconography

## Development

### Scripts

- `npm run dev` - Start development with hot reload
- `npm run start` - Start production build
- `npm run setup` - First-time setup (creates .env file)
- `npm run build:renderer` - Build the React app only
- `npm run build` - Full build with signing (requires certificates)
- `npm run build:mac` - macOS build with signing
- `npm run build:win` - Windows build with signing
- `npm run build:linux` - Linux build
- `npm run pack` - Build without signing (for personal use)
- `npm run dist` - Build and package with signing
- `npm run lint` - Run ESLint
- `npm run preview` - Preview production build

### Architecture

The app consists of two main windows:
1. **Main Window**: Minimal overlay for dictation controls
2. **Control Panel**: Full settings and history interface

Both use the same React codebase but render different components based on URL parameters.

### Key Components

- **main.js**: Electron main process, IPC handlers, database operations
- **preload.js**: Secure bridge between main and renderer processes
- **App.jsx**: Main dictation interface with recording controls
- **ControlPanel.tsx**: Settings, history, and model management
- **whisper_bridge.py**: Python bridge for local Whisper processing
- **better-sqlite3**: Local database for transcription history

### Tailwind CSS v4 Setup

This project uses the latest Tailwind CSS v4 with:
- CSS-first configuration using `@theme` directive
- Vite plugin for optimal performance
- Custom design tokens for consistent theming
- Dark mode support with `@variant`

## Building

The build process creates a single executable for your platform:

```bash
# Development build
npm run pack

# Production builds
npm run dist           # Current platform
npm run build:mac      # macOS DMG + ZIP
npm run build:win      # Windows NSIS + Portable
npm run build:linux    # AppImage + DEB
```

## Configuration

### Environment Variables

Create a `.env` file in the root directory (or use `npm run setup`):

```env
# OpenAI API Configuration (optional - only needed for cloud processing)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Customize the Whisper model
WHISPER_MODEL=whisper-1

# Optional: Set language for better transcription accuracy
LANGUAGE=

# Optional: Anthropic API Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Optional: Google Gemini API Configuration  
GEMINI_API_KEY=your_gemini_api_key_here

# Optional: Debug mode
DEBUG=false
```

### Local Whisper Setup

For local processing, OpenWhispr offers automated setup:

1. **Automatic Python Installation** (if needed):
   - The app will detect if Python is missing
   - Offers to install Python 3.11 automatically
   - macOS: Uses Homebrew if available, otherwise official installer
   - Windows: Downloads and installs official Python
   - Linux: Uses system package manager (apt, yum, or pacman)

2. **Automatic Whisper Setup**:
   - Installs OpenAI Whisper package via pip
   - Downloads your chosen model on first use
   - Handles all transcription locally

**Requirements**:
- Sufficient disk space for models (39MB - 1.5GB depending on model)
- Admin/sudo access may be required for Python installation

### GPU Acceleration (NVIDIA CUDA)

For significantly faster local transcription, OpenWhispr supports NVIDIA GPU acceleration via CUDA. This can speed up transcription by 5-10x compared to CPU.

**Requirements**:
- NVIDIA GPU with CUDA Compute Capability 6.0+ (GTX 10-series or newer)
- NVIDIA Driver 525+ (for CUDA 12.x support)
- 2GB+ VRAM (4GB+ recommended for larger models)

**Step 1: Install CUDA Toolkit**

```bash
# Ubuntu/Debian
sudo apt install nvidia-cuda-toolkit

# Fedora
sudo dnf install cuda

# Arch Linux
sudo pacman -S cuda

# Or download from NVIDIA: https://developer.nvidia.com/cuda-downloads
```

**Step 2: Install cuDNN (CUDA Deep Neural Network library)**

cuDNN is required for GPU inference with faster-whisper/CTranslate2:

```bash
# Ubuntu/Debian (recommended - installs cuDNN 9.x)
sudo apt install libcudnn9-cuda-12 libcudnn9-dev-cuda-12

# Or for cuDNN 8.x (older systems)
sudo apt install libcudnn8 libcudnn8-dev

# Fedora
sudo dnf install cudnn

# Arch Linux
sudo pacman -S cudnn

# Alternative: Install via pip (adds cuDNN to Python environment)
pip install nvidia-cudnn-cu12
# or with uv:
uv pip install nvidia-cudnn-cu12
```

**Step 3: Install PyTorch with CUDA Support**

The default PyTorch installation is CPU-only. Install the CUDA version:

```bash
# Using pip (CUDA 12.4 - recommended for latest GPUs)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# Using uv (faster)
uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# For CUDA 12.1 (more compatible)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# For CUDA 11.8 (older systems)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Pre-release/nightly builds (latest features)
uv pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124
```

**Step 4: Install faster-whisper with GPU support**

```bash
pip install faster-whisper
# or with uv:
uv pip install faster-whisper
```

**Verify GPU Setup**:

```bash
# Check CUDA availability
python3 -c "import torch; print('CUDA:', torch.cuda.is_available(), '| GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')"

# Check cuDNN availability
python3 -c "import ctypes; ctypes.CDLL('libcudnn.so.9'); print('cuDNN: OK')"

# Expected output:
# CUDA: True | GPU: NVIDIA GeForce RTX 5090
# cuDNN: OK
```

**Troubleshooting GPU Issues**:

| Issue | Solution |
|-------|----------|
| `CUDA not available` | Install NVIDIA drivers and CUDA toolkit |
| `cuDNN not found` | Install libcudnn packages or `pip install nvidia-cudnn-cu12` |
| `Out of memory` | Use a smaller model (base instead of large-v3) |
| `Slow GPU inference` | Ensure cuDNN is installed (required for CTranslate2 optimization) |

**GPU Memory Usage by Model**:

| Model | VRAM Usage | Speed (RTX 3080) |
|-------|-----------|------------------|
| tiny | ~1 GB | ~30x realtime |
| base | ~1.5 GB | ~25x realtime |
| small | ~2.5 GB | ~15x realtime |
| medium | ~5 GB | ~8x realtime |
| large-v3 | ~10 GB | ~5x realtime |
| turbo | ~6 GB | ~12x realtime |
| distil-large-v3 | ~4 GB | ~20x realtime |

> **Note**: OpenWhispr automatically detects GPU availability and uses it when possible. If GPU inference fails, it falls back to CPU. The model is preloaded into GPU memory at app startup for instant transcription.

### Customization

- **Hotkey**: Change in the Control Panel (default: backtick `) - fully customizable
- **Panel Position**: Drag the dictation panel to any location on your screen`
- **Processing Method**: Choose local or cloud in Control Panel
- **Whisper Model**: Select quality vs speed in Control Panel
- **UI Theme**: Edit CSS variables in `src/index.css`
- **Window Size**: Adjust dimensions in `main.js`
- **Database**: Transcriptions stored in user data directory

## Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Guidelines

- Run `npm run lint` before committing
- Follow the existing code style
- Update documentation as needed
- Test on your target platform before submitting
## Security

OpenWhispr is designed with privacy and security in mind:

- **Local Processing Option**: Keep your voice data completely private
- **No Analytics**: We don't collect any usage data or telemetry
- **Open Source**: All code is available for review
- **Secure Storage**: API keys are stored securely in your system's keychain/credential manager
- **Minimal Permissions**: Only requests necessary permissions (microphone, accessibility)

## Troubleshooting

### Common Issues

1. **Microphone permissions**: Grant permissions in System Preferences/Settings
2. **Accessibility permissions (macOS)**: Required for automatic text pasting
   - Go to System Settings â†’ Privacy & Security â†’ Accessibility
   - Add OpenWhispr and enable the checkbox
   - Use "Fix Permission Issues" in Control Panel if needed
3. **API key errors** (cloud processing only): Ensure your OpenAI API key is valid and has credits
   - Set key through Control Panel or .env file
   - Check logs for "OpenAI API Key present: Yes/No"
4. **Local Whisper installation**: 
   - Ensure Python 3.7+ is installed
   - Use Control Panel to install Whisper automatically
   - Check available disk space for models
5. **Global hotkey conflicts**: Change the hotkey in the Control Panel - any key can be used
6. **Text not pasting**: Check accessibility permissions and try manual paste with Cmd+V
7. **Panel position**: If the panel appears off-screen, restart the app to reset position

### Getting Help

- Check the [Issues](https://github.com/your-repo/open-whispr/issues) page
- Review the console logs for debugging information
- For local processing: Ensure Python and pip are working
- For cloud processing: Verify your OpenAI API key and billing status
- Check the Control Panel for system status and diagnostics

### Performance Tips

- **Local Processing**: Use "base" model for best balance of speed and accuracy
- **Cloud Processing**: Generally faster but requires internet connection
- **Model Selection**: tiny (fastest) â†’ base (recommended) â†’ small â†’ medium â†’ large (best quality)
- **Permissions**: Ensure all required permissions are granted for smooth operation

## FAQ

**Q: Is OpenWhispr really free?**
A: Yes! OpenWhispr is open source and free to use. You only pay for OpenAI API usage if you choose cloud processing.

**Q: Which processing method should I use?**
A: Use local processing for privacy and offline use. Use cloud processing for speed and convenience.

**Q: Can I use this commercially?**
A: Yes! The MIT license allows commercial use.

**Q: How do I change the hotkey?**
A: Open the Control Panel (right-click tray icon) and go to Settings. You can set any key as your hotkey.

**Q: Is my data secure?**
A: With local processing, your audio never leaves your device. With cloud processing, audio is sent to OpenAI's servers (see their privacy policy).

**Q: What languages are supported?**
A: OpenWhispr supports 58 languages including English, Spanish, French, German, Chinese, Japanese, and more. Set your preferred language in the .env file or use auto-detect.

## Project Status

OpenWhispr is actively maintained and ready for production use. Current version: 1.0.4

- âœ… Core functionality complete
- âœ… Cross-platform support
- âœ… Local and cloud processing
- âœ… Automatic Python/Whisper installation
- âœ… Agent naming system
- âœ… Draggable interface
- ðŸš§ Continuous improvements and bug fixes
